{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b733f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Ensure the Google API key is set as an environment variable.\n",
    "# (Replace 'YOUR_GOOGLE_API_KEY' with your actual key or use another method to supply the key.)\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyADCKnydIYN5CZiYfuNaswxGB5ZjspeOh8\"\n",
    "\n",
    "from pypdf import PdfReader                              # PyPDF for PDF text extraction:contentReference[oaicite:4]{index=4}\n",
    "import easyocr                                           # EasyOCR for image (JPG) text extraction:contentReference[oaicite:5]{index=5}:contentReference[oaicite:6]{index=6}\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_google_genai import GoogleGenerativeAI, ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores.faiss import FAISS    # FAISS vector store for embeddings:contentReference[oaicite:7]{index=7}\n",
    "import gradio as gr\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "b3f3f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = None\n",
    "llm = None\n",
    "chat_llm = None\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "vector_store = None\n",
    "qa_chain = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "24d5e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models():\n",
    "    \"\"\"Initialize the Google Gemini models with error handling.\"\"\"\n",
    "    global embed_model, llm, chat_llm\n",
    "    try:\n",
    "        print(\"Initializing Google Gemini models...\")\n",
    "        embed_model = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "        llm = GoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "        chat_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "        print(\"Models initialized successfully!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing models: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a18bbc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"Extract text from a PDF file using PyPDF.\"\"\"\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d102c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(file_path):\n",
    "    \"\"\"Extract English text from an image (JPG) using EasyOCR.\"\"\"\n",
    "    reader = easyocr.Reader(['en'], gpu=False)\n",
    "    # detail=0 returns only the detected text strings:contentReference[oaicite:11]{index=11}\n",
    "    results = reader.readtext(file_path, detail=0)\n",
    "    text = \" \".join(results)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1ae4d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(uploaded_file):\n",
    "    \"\"\"Handle file upload: extract text, split into chunks, embed, build FAISS index, and compute summary and clauses.\"\"\"\n",
    "    global vector_store, qa_chain, embed_model, llm, chat_llm\n",
    "    \n",
    "    # Initialize models if not already done\n",
    "    if embed_model is None or llm is None or chat_llm is None:\n",
    "        if not initialize_models():\n",
    "            return \"Error: Could not initialize AI models. Please check your Google API key.\", \"\"\n",
    "    \n",
    "    try:\n",
    "        file_path = uploaded_file.name\n",
    "        \n",
    "        # Determine file type by extension\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "        elif file_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            text = extract_text_from_image(file_path)\n",
    "        else:\n",
    "            return \"Unsupported file format\", \"\"\n",
    "        \n",
    "        if not text.strip():\n",
    "            return \"No text could be extracted from the file.\", \"\"\n",
    "        \n",
    "        # Split text into chunks for embedding and retrieval\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        \n",
    "        # Convert chunks into Documents for summarization\n",
    "        documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "        \n",
    "        # Create or recreate the FAISS vector store with Gemini embeddings\n",
    "        vector_store = FAISS.from_texts(chunks, embedding=embed_model)\n",
    "        \n",
    "        # Create a Retriever for RAG (could use max marginal relevance for diversity)\n",
    "        retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "        \n",
    "        # Build the RetrievalQA chain using the chat LLM\n",
    "        from langchain.chains import RetrievalQA\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=chat_llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=False\n",
    "        )\n",
    "        \n",
    "        # Summarize the document using LangChain's summarize chain (map-reduce for large docs)\n",
    "        from langchain.chains.summarize import load_summarize_chain\n",
    "        summarize_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "        \n",
    "        # FIX: Pass documents directly, not as invoke() parameter\n",
    "        summary_result = summarize_chain.run(documents)\n",
    "        \n",
    "        # Extract summary text properly\n",
    "        if isinstance(summary_result, dict):\n",
    "            summary = summary_result.get('output_text', str(summary_result))\n",
    "        else:\n",
    "            summary = str(summary_result)\n",
    "        \n",
    "        # Identify main clauses using the RAG QA chain\n",
    "        clause_query = \"List the main clauses in the document and briefly describe each clause.\"\n",
    "        \n",
    "        # FIX: Use run() method for RetrievalQA chain\n",
    "        try:\n",
    "            clauses_result = qa_chain.run(clause_query)\n",
    "            clauses = str(clauses_result)\n",
    "        except Exception as clause_error:\n",
    "            print(f\"Error getting clauses: {clause_error}\")\n",
    "            clauses = \"Could not extract clauses from the document.\"\n",
    "        \n",
    "        return summary, clauses\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Detailed error: {str(e)}\")\n",
    "        return f\"Error processing file: {str(e)}\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7597ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def answer_question(user_input, chat_history):\n",
    "#     \"\"\"Answer a user question by retrieving relevant chunks and using the QA chain.\"\"\"\n",
    "#     global qa_chain, embed_model, llm, chat_llm\n",
    "    \n",
    "#     # Initialize models if not already done\n",
    "#     if embed_model is None or llm is None or chat_llm is None:\n",
    "#         if not initialize_models():\n",
    "#             bot_response = \"Error: Could not initialize AI models. Please check your Google API key.\"\n",
    "#         else:\n",
    "#             bot_response = \"Please upload a document first.\"\n",
    "#     elif not qa_chain:\n",
    "#         # No document has been uploaded yet\n",
    "#         bot_response = \"Please upload a document first.\"\n",
    "#     else:\n",
    "#         try:\n",
    "#             # Append user message to chat history and query the QA chain\n",
    "#             bot_response = qa_chain.invoke({\"input\": user_input, \"chat_history\": chat_history or []})\n",
    "#         except Exception as e:\n",
    "#             bot_response = f\"Error processing question: {str(e)}\"\n",
    "\n",
    "#     # Update chat history: list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "#     chat_history = chat_history or []\n",
    "#     chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "#     chat_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "#     # Clear the input box by returning \"\" for user_input\n",
    "#     return \"\", chat_history\n",
    "def answer_question(user_input, chat_history):\n",
    "    \"\"\"Answer questions using the RAG chain with proper chat history handling.\"\"\"\n",
    "    global qa_chain\n",
    "    \n",
    "    if not qa_chain:\n",
    "        return \"Please upload a document first.\", chat_history or []\n",
    "    \n",
    "    try:\n",
    "        # Use run() method for simple question answering\n",
    "        bot_response = qa_chain.run(user_input)\n",
    "        \n",
    "    except Exception as e:\n",
    "        bot_response = f\"Error processing question: {e}\"\n",
    "    \n",
    "    # Update chat history\n",
    "    chat_history = chat_history or []\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "    \n",
    "    return bot_response, chat_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "9f821d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Legal Document Analysis App...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    [Errno 48] error while attempting to bind on address ('0.0.0.0', 7878): [errno 48] address already in use\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot find empty port in range: 7878-7878. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[229]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m     msg.submit(answer_question, [msg, chatbot], [msg, chatbot])\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Legal Document Analysis App...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mdemo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshare\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m0.0.0.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7878\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LegalDoc/venv/lib/python3.13/site-packages/gradio/blocks.py:2758\u001b[39m, in \u001b[36mBlocks.launch\u001b[39m\u001b[34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001b[39m\n\u001b[32m   2750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2751\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m http_server\n\u001b[32m   2753\u001b[39m     (\n\u001b[32m   2754\u001b[39m         server_name,\n\u001b[32m   2755\u001b[39m         server_port,\n\u001b[32m   2756\u001b[39m         local_url,\n\u001b[32m   2757\u001b[39m         server,\n\u001b[32m-> \u001b[39m\u001b[32m2758\u001b[39m     ) = \u001b[43mhttp_server\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2766\u001b[39m \u001b[38;5;28mself\u001b[39m.server_name = server_name\n\u001b[32m   2767\u001b[39m \u001b[38;5;28mself\u001b[39m.local_url = local_url\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/LegalDoc/venv/lib/python3.13/site-packages/gradio/http_server.py:156\u001b[39m, in \u001b[36mstart_server\u001b[39m\u001b[34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[39m\n\u001b[32m    154\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot find empty port in range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m     )\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ssl_keyfile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    161\u001b[39m     path_to_local_server = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_host_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: Cannot find empty port in range: 7878-7878. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`."
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Legal Document Analysis with Google Gemini\")\n",
    "    gr.Markdown(\n",
    "        \"Upload a PDF or image of a legal document. The app will summarize the document, identify its main clauses, \"\n",
    "        \"and allow you to ask questions about the content.\"\n",
    "    )\n",
    "    \n",
    "    # Status indicator\n",
    "    status_text = gr.Textbox(label=\"Status\", value=\"Ready to process documents. Models will be initialized when needed.\", interactive=False)\n",
    "    \n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload Document (PDF or JPG)\", type=\"filepath\")\n",
    "        summary_box = gr.Textbox(label=\"Document Summary\", lines=5)\n",
    "        clauses_box = gr.Textbox(label=\"Identified Clauses\", lines=5)\n",
    "        file_input.change(\n",
    "            fn=process_file,\n",
    "            inputs=[file_input],\n",
    "            outputs=[summary_box, clauses_box]\n",
    "        )\n",
    "\n",
    "    # Chat-style Q&A interface\n",
    "    gr.Markdown(\"## Ask Questions\")\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Document\", type=\"messages\")\n",
    "    msg = gr.Textbox(placeholder=\"Enter your question about the document...\")\n",
    "    msg.submit(answer_question, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "print(\"Starting Legal Document Analysis App...\")\n",
    "demo.launch(share=False, server_name=\"0.0.0.0\", server_port=7878)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab85559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DOCUMENT SUMMARY ===\n",
      "\n",
      "Error processing file: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.\n",
      "\n",
      "=== IDENTIFIED CLAUSES ===\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class MockUpload:\n",
    "    def __init__(self, path):\n",
    "        self.name = path\n",
    "\n",
    "# Set your API key\n",
    "os.environ.setdefault(\"GOOGLE_API_KEY\", \"AIzaSyADCKnydIYN5CZiYfuNaswxGB5ZjspeOh8\")\n",
    "\n",
    "# Test the function\n",
    "local_path = \"1.pdf\"\n",
    "mock_file = MockUpload(local_path)\n",
    "\n",
    "# Try the main function first\n",
    "summary, clauses = process_file(mock_file)\n",
    "print(\"=== DOCUMENT SUMMARY ===\\n\")\n",
    "print(summary)\n",
    "print(\"\\n=== IDENTIFIED CLAUSES ===\\n\")\n",
    "print(clauses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
